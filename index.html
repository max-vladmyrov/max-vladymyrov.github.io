<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
        "http://www.w3.org/TR/html401">
<html>
	<meta charset="utf-8" />
  <head>
<base href="https://max-vladymyrov.github.io" />

    <title>Max Vladymyrov</title>
    <link rev="made" href="mailto:vladymyrov at gmail dot com">
    <link rel="stylesheet" type="text/css" href="html-style.css">
    <meta name="author" content="Max Vladymyrov">
    <meta name="copyright" content="&copy; 2012 Max Vladymyrov">
    <meta http-equiv="Content-language" content="en">
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    <meta name="description" content="Home page of Max Vladymyrov">
    <meta name="keywords" content="Max, Vladymyrov, mvladymyrov, mxv, Google">
    <meta name="robots" content="all">
  </head>
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-QQMENH7L2Y"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-QQMENH7L2Y');
	</script>
  
  <body>
    
    <table border="0" cellspacing="0">
      <tbody>
	<tr>
	  <td align="left"><img height="221" border="0" src="max.png" alt=""></td>
	  <td width="20">&nbsp;</td>
	  <td align="left" nowrap>
	    <h2>Max Vladymyrov / &#1052;&#1072;&#1082;&#1089;&#1080;&#1084; &#1042;&#1083;&#1072;&#1076;&#1080;&#1084;&#1080;&#1088;&#1086;&#1074;</h2>
	    <samp>
	      Senior Research Scientist, Google DeepMind<br>
	      vladymyrov [at] gmail.com
	    </samp><br><br><br><br>
	  </td>
	</tr>
      </tbody>
    </table>
<table border="0" cellspacing="0">
	<tbody><tr>
	<td>	
	<h2>About</h2>
	I am a Senior Research Scientist working at Google DeepMind. Before that I spent almost two years working at Yahoo Labs. Before that I did my PhD studies at <a href="http://eecs.ucmerced.edu/">UC Merced</a> on the problems of large-scale dimensionality reduction under the supervision of <a href="http://faculty.ucmerced.edu/mcarreira-perpinan/"> Miguel Á. Carreira-Perpiñán</a>. I received two MS in Computer Science and in International Economic Relations and BS in Applied Math all from <a href="http://www.univer.kharkov.ua/en"> Kharkiv National University</a> in Ukraine. You can find my CV over <a href="cv_vladymyrov.pdf">here</a>.
	<p>
	My work focuses primarily on developing novel machine learning architectures that advance in-context learning and mechanistic interpretability. My long-term research goals include developing highly efficient and interpretable architectures that significantly enhance the capabilities of in-context learning. This will enable researchers to create more adaptable, trustworthy, and powerful AI systems with broader real-world applications.
	
    <h2>Publications <a href="https://scholar.google.com/citations?user=pQZCrqcAAAAJ&hl=en"> <img width="32" height="32" border="0" src="Google-Scholar-favicon.ico" alt="Google Scholar"> </a></h2>
    <ul>
<li><p>Vladymyrov, M., Von Oswald, J., Miller, N., Sandler, M. (2024):
"Efficient Linear System Solver with Transformers".<br><em>ICML 2024 AI for Math Workshop</em>.<br></p></li>
<li><p>Vladymyrov, M., von Oswald, J., Sandler, M., Ge, R. (2024):
"Linear Transformers are Versatile In-Context Learners".<br><em>ICML 2024 Workshop on In-Context Learning</em>.<br></p></li>
<li><p>Zhmoginov, A., Lee, J., Vladymyrov, M., Sandler, M. (2024):
"Learning Fast and Slow: Representations for In-Context Weight Modulation".<br><em>ICML 2024 Workshop on In-Context Learning</em>.<br></p></li>
<li><p>von Oswald, J., Niklasson, E., Schlegel, M., Kobayashi, S., Zucchet, N., Scherrer, N., Miller, N., Sandler, M., Vladymyrov, M., Pascanu, R., Sacramento, J. (2023):
"Uncovering mesa-optimization algorithms in transformers".<br><em>arXiv:2309.05858</em>.<br>
[<a href="https://arxiv.org/abs/2309.05858">arxiv preprint</a>]<br></p></li>
<li><p>von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., Vladymyrov, M. (2023):
"Transformers learn in-context by gradient descent".<br><em>International Conference on Machine Learning (ICML 2023)</em>.<br>
[<a href="https://arxiv.org/abs/2212.07677">arxiv preprint</a>]<br></p></li>
<li><p>Vladymyrov, M., Zhmoginov, A., Sandler, M. (2024):
"Continual Few-Shot Learning Using HyperTransformers".<br><em>Transactions on Machine Learning Research</em>.<br>
[<a href="https://arxiv.org/abs/2301.04584">arxiv preprint</a>]<br></p></li>
<li><p>Zhmoginov, A., Sandler, M., Vladymyrov, M. (2022):
"HyperTransformer: Model generation for supervised and semi-supervised few-shot learning".<br><em>International Conference on Machine Learning (ICML 2022)</em>.<br>
[<a href="https://arxiv.org/abs/2201.04182">arxiv preprint</a>]<br></p></li>
<li><p>Sandler, M., Zhmoginov, A., Vladymyrov, M., Miller, N. (2023):
"Training trajectories, mini-batch losses and the curious role of the learning rate".<br><em>arXiv preprint arXiv:2301.02312</em>.<br>
[<a href="https://arxiv.org/abs/2301.02312">arxiv preprint</a>]<br></p></li>
<li><p>Evci, U., van Merrienboer, B., Unterthiner, T., Vladymyrov, M., Pedregosa, F. (2022):
"Gradmax: Growing neural networks using gradient information".<br><em>International Conference on Learning Representations (ICLR 2022)</em>.<br>
[<a href="https://arxiv.org/abs/2201.05125">arxiv preprint</a>]<br></p></li>
<li><p>Sandler, M., Zhmoginov, A., Vladymyrov, M., Jackson, A. (2022):
"Fine-tuning image transformers using learnable memory".<br><em>Computer Vision and Pattern Recognition (CVPR 2022)</em>.<br>
[<a href="https://arxiv.org/abs/2203.15243">arxiv preprint</a>]<br></p></li>
<li><p>Zhmoginov, A., Sandler, M., Miller, N., Kristiansen, G., Vladymyrov, M. (2023):
"Decentralized learning with multi-headed distillation".<br><em>Computer Vision and Pattern Recognition (CVPR 2023)</em>.<br>
[<a href="https://arxiv.org/abs/2211.15774">arxiv preprint</a>]<br></p></li>
<li><p>Sandler, M., Vladymyrov, M., Zhmoginov, A., Miller, N., Jackson, A., Madams, T. and Agüera y Arcas, B. (2021):
"Meta-learning bidirectional update rules".<br><em>38th International Conference on Machine Learning  (ICML 2021)</em>, pp. 9288-9300.<br>
  [<a href="https://arxiv.org/abs/2104.04657">arxiv preprint</a>] [<a href="papers/BLUR_poster.pdf">poster</a>] [<a href="papers/BLUR_ICML_presentation.pdf">slides</a>] [<a href="https://github.com/google-research/google-research/tree/master/blur">GitHub</a>]<br></p></li>
<li><p>Vladymyrov, M. (2019): "No pressure! Addressing the problem of local minima in manifold learning algorithms". <br><em> 33th Annual Conference on Neural Information Processing Systems (NeurIPS 2019)</em>, pp. 678-687.<br>
  [<a href="https://arxiv.org/abs/1906.11389">arxiv preprint</a>] [<a href="pp_poster.pdf">poster</a>] [<a href="pp_slides.pdf">slides</a>] [<a href="pp_code.zip">Matlab code</a>]<br></p></li>
<li><p>Vladymyrov, M. and Carreira-Perpiñán, M. Á. (2017): "Fast, accurate spectral clustering using locally linear landmarks". <br><em> 30th International Joint Conference on Neural Networks (IJCNN 2017)</em>, pp. 3870-3879.<br>
  [<a href="http://www.ijcnn.org/">external link</a>] [<a href="papers/ijcnn17.pdf">paper preprint</a>] [<a href="papers/ijcnn17-slides.pdf">slides</a>]<br></p></li>
	<li><p>Vladymyrov, M. and Carreira-Perpiñán, M. Á. (2016): "The variational Nyström method for large-scale spectral problems". <br><em> 33th International Conference on Machine Learning  (ICML 2016)</em>, pp. 211-220.<br>
			  [<a href="http://jmlr.org/proceedings/papers/v48/vladymyrov16.html">external link</a>] [<a href="papers/icml16.pdf">paper preprint</a>] [<a href="papers/icml16-supp.pdf">supplementary</a>] [<a href="http://icml.cc/2016/reviews/128.txt">reviews</a>] [<a href="papers/icml16-poster.pdf">poster</a>] [<a href="papers/icml16-slides.pdf">slides</a>] [<a href="vn.tar.gz">Matlab code</a>]<br>
				<small> &#9656; This paper addresses the problem of fast approximate spectral methods and presents a variation of a Nyström algorithm that is more justified in that context.
	</small></p></li><li><p>Carreira-Perpiñán, M. Á. and Vladymyrov, M. (2015): "A fast, universal algorithm to learn parametric nonlinear embeddings". <br><em> 29th Annual Conference on Neural Information Processing Systems (NIPS 2015)</em>, pp. 253-261.<br>
			  [<a href="https://nips.cc/Conferences/2015/AcceptedPapers">external link</a>]  [<a href="papers/nips15.pdf">paper preprint</a>] [<a href="papers/nips15-supp.zip">supplementary</a>] [<a href="https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips28/reviews/134.html">reviews</a>] [<a href="papers/nips15-poster.pdf">poster</a>] [<a href="pe_mac.zip">Matlab code</a>]<br>
	<small>&#9656; This uses the method of auxiliary coordinates (MAC) to learn an optimal mapping (such as linear or a neural net) for a nonlinear embedding (such as the elastic embedding or t-SNE).</small><br>
	<small>&#9656; This work was also presented at <a href="http://2014.baylearn.org/">BayLearn 2014</a> [<a href="papers/baylearn14b.pdf">extended abstract</a>] [<a href="papers/baylearn14b-poster.pdf">poster</a>] </small>
	</p></li>
	<li><p>Vladymyrov, M. (2014): "Large-scale methods for nonlinear manifold learning", <br><em>PhD thesis, Electrical Engineering and Computer Science, University of California, Merced</em>.<br>
			  [<a href="http://escholarship.org/uc/item/9hj5v8z2">external link</a>] [<a href="papers/vladymyrov_thesis.pdf">paper</a>] [<a href="papers/vladymyrov_thesis_slides.pdf">defense slides</a>]</p></li>
				
	<li><p>Vladymyrov, M. and Carreira-Perpiñán, M. Á. (2014): "Linear-time training of nonlinear low-dimensional embeddings". <br><em> 17th International Conference on Artificial Intelligence and Statistics (AISTATS 2014)</em>, pp. 968-977.<br>
			  [<a href="http://jmlr.org/proceedings/papers/v33/vladymyrov14.html">external link</a>] [<a href="papers/aistats14b.pdf">paper preprint</a>] [<a href="papers/aistats14-supp/index.html">supplementary</a>] [<a href="papers/aistats14-poster.pdf">poster</a>] [<a href="NBody_NLE.tar.gz">Matlab code</a>]<br>
	<small>&#9656; This paper applies <em>fast multipole methods</em> and <em>Barnes-Hut</em> approximation to nonlinear embedding methods, allowing to get fast embedding of datasets with more than a million points. </small><br>
	<small>&#9656; Check out the <a href="papers/aistats14-supp/infiniteMNIST_markers.mov">animation</a> of optimizing one million MNIST digits. Every marker corresponds to one MNIST digit. <a href="papers/aistats14-supp/infiniteMNIST_digits.mov">Here</a> is the same animations, but subsampled and visualized using actual images of the digits.</small><br>
	<small>&#9656; This work was also presented at <a href="http://2013.baylearn.org/">BayLearn 2013</a> [<a href="papers/bamls13a.pdf">extended abstract</a>] [<a href="papers/bamls13-poster.pdf">poster</a>]<br></small>
	</p></li>
			
	<li><p>Vladymyrov, M. and Carreira-Perpiñán, M. Á. (2013): "Locally linear landmarks for large-scale manifold learning". <br><em> 24th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2013)</em>, pp. 256-271.<br>
		  [<a href="http://dx.doi.org/10.1007/978-3-642-40994-3_17">external link</a>] [<a href="papers/ecml13.pdf">paper preprint</a>] [<a href="papers/ecml13-poster.pdf">poster</a>] [<a href="papers/ecml13-slides.pdf">slides</a>] [<a href="LLL.tar.gz">Matlab code</a>]<br>
	<small>&#9656; This paper defines <em>locally linear landmarks (LLL)</em> algorithm, a method to scale-up spectral method algorithms (such as Kernel PCA, Laplacian Eigenmaps, Spectral Clustering) using landmarks selected from data. </small><br>
	<small>&#9656; This work was also presented at <a href="https://sites.google.com/site/spectralworkshopicml2013/posters">Spectral Learning Workshop</a> at ICML 2013 [<a href="papers/icml13-spectral-workshop.pdf">paper preprint</a>] [<a href="papers/icml13-spectral-workshop-poster.pdf">poster</a>]<br>
	</small></p></li>

	<li><p>Vladymyrov, M. and Carreira-Perpiñán, M. Á. (2013): "Entropic affinities: properties and efficient numerical computation". <br><em> 30th International Conference on Machine Learning  (ICML 2013)</em>, pp. 477-485.<br>[<a href="http://jmlr.org/proceedings/papers/v28/vladymyrov13.html">external link</a>] [<a href="papers/icml13.pdf">paper preprint</a>] [<a href="papers/icml13-supp/index.html">supplementary</a>] [<a href="papers/icml13-slides.pdf">slides</a>] [<a href="http://techtalks.tv/talks/entropic-affinities-properties-and-efficient-numerical-computation/58285/">video</a>] [<a href="papers/icml13-poster.pdf">poster</a>] [<a href="ea.tar.gz">Matlab code</a>]<br>
	<small>&#9656; This introduces the <em>entropic affinities</em>, a fast method to compute variable-bandwidth affinity matrix. This method should give better results than fixing bandwidth &#963; to a single value. </small>	
	</p></li>
	
	<li><p>Vladymyrov, M. and Carreira-Perpiñán, M. Á. (2012): "Partial-Hessian strategies for fast learning of nonlinear embeddings". <br><em> 29th International Conference on Machine Learning  (ICML 2012)</em>, pp. 345-352.<br> [<a href="http://icml.cc/2012/papers/199.pdf">external link</a>] [<a href="papers/icml12.pdf">paper preprint</a>] [<a href="papers/icml12-supp/index.html">supplementary</a>] [<a href="papers/icml12-slides.pdf">slides</a>] [<a href="http://techtalks.tv/talks/fast-training-of-nonlinear-embedding-algorithms/57511/">video</a>] [<a href="papers/icml12-poster.pdf">poster</a>] [<a href="code-EE_SNE_tSNE.tar.gz">Matlab code</a>]<br>
	  <small>&#9656; This introduces the <em>spectral direction</em>, a fast optimization method for nonlinear embedding methods. This code is the fastest one available for EE, SNE and t-SNE as far as I know.</small><br>
		<small>&#9656; This work was also presented at <a href="http://2012.baylearn.org/">BayLearn 2012</a> [<a href="papers/baylearn12a.pdf">extended abstract</a>] [<a href="papers/baylearn12a-slides.pdf">slides</a>]<br></small>
	</p></li>
    </ul>


<h2>Social media</h2>

[<a href="https://twitter.com/mvladymyrov">Twitter</a>] [<a href="https://www.linkedin.com/in/max-vladymyrov/">LinkedIn</a>]



</td></tr>
</tbody></table></body>
</html>
