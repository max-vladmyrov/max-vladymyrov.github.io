<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Max Vladymyrov, Senior Research Scientist at Google DeepMind, specializing in in-context learning and mechanistic interpretability in AI and machine learning.">
    <title>Max Vladymyrov - Senior Research Scientist</title>
    <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
	
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Max Vladymyrov",
      "jobTitle": "Senior Research Scientist",
      "worksFor": {
        "@type": "Organization",
        "name": "Google DeepMind"
      },
      "alumniOf": {
        "@type": "CollegeOrUniversity",
        "name": "UC Merced"
      },
      "url": "https://max-vladymyrov.github.io/",
      "sameAs": [
        "https://twitter.com/mvladymyrov",
        "https://www.linkedin.com/in/max-vladymyrov/",
        "https://scholar.google.com/citations?user=pQZCrqcAAAAJ&hl=en"
      ]
    }
    </script>	
    <style>
		html {
		    scroll-behavior: smooth;
		}
        body {
            font-family: 'Open Sans', sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
		.header {
		    display: flex;
		    align-items: center;
		    padding: 20px;
		    background-color: #f9f9f9;
		    border-radius: 10px;
		    margin-bottom: 30px;
		}
		.header img {
		    width: 150px;
		    height: 150px;
		    border-radius: 50%;
		    margin-right: 30px;
		    object-fit: cover;  
		    object-position: center; 
		}

		.header-text h1 {
		    margin: 0 0 10px 0;
		    color: #2c3e50;
		}
		.header-text p {
		    margin: 0;
		    color: #7f8c8d;
		}
        .social-icons a {
            color: #3498db;
            font-size: 1.5rem;
            margin-right: 1rem;
        }
        .about {
            margin-bottom: 2rem;
        }
        .about h2 {
            color: #2c3e50;
        }
        .cta-button {
            display: inline-block;
            background-color: #3498db;
            color: white;
            padding: 0.5rem 1rem;
            text-decoration: none;
            border-radius: 5px;
            margin-top: 1rem;
        }
	    .publications {
	        margin-bottom: 2rem;
	    }
		.filter-btn {
		    padding: 8px 16px;
		    border: none;
		    border-radius: 20px;
		    background-color: #f0f0f0;
		    color: #333;
		    cursor: pointer;
		    transition: all 0.3s ease;
		    margin-right: 10px;
		    margin-bottom: 10px;
		}
		.filter-button {
		  padding: 8px 16px;
		  border: 1px solid #ccc;
		  border-radius: 20px;
		  background-color: #f0f0f0;
		  cursor: pointer;
		  transition: background-color 0.3s;All Years
		}
		.filter-buttons {
		  display: flex;
		  flex-wrap: wrap;
		  gap: 10px;
		  margin-bottom: 20px;
		}
		.filter-btn:hover {
		    background-color: #e0e0e0;
		}

		.filter-btn.active {
		    background-color: #3498db;
		    color: white;
		}
		.year-buttons, .topic-buttons, .venue-buttons {
		  display: flex;
		  flex-wrap: wrap;
		  gap: 8px;
		  margin-bottom: 15px;
		}

		#all-button {
		  font-weight: bold;
		  background-color: #007bff;
		  color: white;
		}
	    .publication-grid {
	        display: grid;
	        grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
	        gap: 1rem;
	    }
		.publication-card {
		    position: relative;
		    padding: 1rem;
		    border: 1px solid #e0e0e0;
		    border-radius: 5px;
		    margin-bottom: 1rem;
		    display: flex;
		    flex-direction: column;
		    min-height: 200px;
		}

		.publication-card h3 {
		    margin-top: 25px; /* Add margin to the top of the title */
		    margin-bottom: 0.5rem;
		    line-height: 1.3;
		}
		.authors, .venue {
		    font-size: 0.9rem;
		    color: #666;
		    margin-bottom: 0.5rem;
		}
		.publication-links {
		    margin-top: auto;
			margin-bottom: 60px;
		    display: flex;
		    flex-wrap: wrap;
		    gap: 10px;
		}
		.pub-link {
		    padding: 5px 10px;
		    background-color: #f0f0f0;
		    border-radius: 15px;
		    font-size: 0.9em;
		    text-decoration: none;
		    color: #333;
		    transition: background-color 0.3s ease;
		}
		.pub-link:hover {
		    background-color: #e0e0e0;
		}
		.pub-link i {
		    margin-right: 4px;
		}
		.research-interests, .current-projects {
	        margin-bottom: 2rem;
	    }
	    .project-card {
	        background-color: #f9f9f9;
	        padding: 1rem;
	        border-radius: 5px;
	        margin-bottom: 1rem;
	    }
	    .contact form {
	        display: flex;
	        flex-direction: column;
	    }
	    .contact input, .contact textarea {
	        margin-bottom: 1rem;
	        padding: 0.5rem;
	    }
	    .contact button {
	        background-color: #3498db;
	        color: white;
	        border: none;
	        padding: 0.5rem;
	        cursor: pointer;
	    }
	    footer {
	        margin-top: 2rem;
	        border-top: 1px solid #e0e0e0;
	        padding-top: 1rem;
	    }
	    .footer-content {
	        display: flex;
	        justify-content: space-between;
	        align-items: center;
	    }
	    #back-to-top {
	        position: fixed;
	        bottom: 20px;
	        right: 20px;
	        background-color: #3498db;
	        color: white;
	        border: none;
	        border-radius: 50%;
	        width: 40px;
	        height: 40px;
	        font-size: 1.2rem;
	        cursor: pointer;
	        display: none;
	    }
		.filter-btn[data-filter^="tag-"] {
		    background-color: #e0e0e0;
		    color: #333;
		}

		.filter-btn[data-filter^="tag-"].active {
		    background-color: #2980b9;
		    color: white;
		}
		.tags {
		    position: absolute;
		    bottom: 0.5rem;
		    left: 1rem;
		    right: 1rem;
		    display: flex;
		    flex-wrap: wrap;
		    gap: 5px;
		}

		.tag {
		    background-color: #e0e0e0;
		    color: #333;
		    padding: 2px 8px;
		    border-radius: 12px;
		    font-size: 0.8rem;
		    white-space: nowrap;
		}
		.venue-tag {
		    position: absolute;
		    top: 0.5rem;
		    right: 0.5rem;
		    background-color: #3498db;
		    color: white;
		    padding: 2px 8px;
		    font-size: 0.8rem;
		    border-radius: 12px;
		}
		.filter-group {
		    margin-bottom: 1rem;
		}

		.filter-group h4 {
		    margin-bottom: 0.5rem;
		    font-size: 1rem;
		    color: #555;
		}

		.button-row {
		    display: flex;
		    overflow-x: auto;
		    padding-bottom: 10px;
		    margin-bottom: 10px;
		    -webkit-overflow-scrolling: touch;
		}

		.publication-filters {
		    margin-bottom: 2rem;
		}

		.filter-group {
		    margin-bottom: 1rem;
		}

		.filter-group h4 {
		    margin-bottom: 0.5rem;
		    font-size: 1rem;
		    color: #555;
		    font-weight: 600;
		}

		.button-row {
		    display: flex;
		    overflow-x: auto;
		    padding-bottom: 10px;
		    margin-bottom: 10px;
		    -webkit-overflow-scrolling: touch;
		}

		.filter-btn {
		    flex: 0 0 auto;
		    background-color: #f0f0f0;
		    border: none;
		    border-radius: 20px;
		    padding: 8px 16px;
		    margin-right: 10px;
		    cursor: pointer;
		    transition: background-color 0.3s, color 0.3s;
		    font-size: 0.9rem;
		    white-space: nowrap;
		}

		.filter-btn:hover {
		    background-color: #e0e0e0;
		}

		.filter-btn.active {
		    background-color: #3498db;
		    color: white;
		}

		/* Hide scrollbar for Chrome, Safari and Opera */
		.button-row::-webkit-scrollbar {
		    display: none;
		}

		/* Hide scrollbar for IE, Edge and Firefox */
		.button-row {
		    -ms-overflow-style: none;  /* IE and Edge */
		    scrollbar-width: none;  /* Firefox */
		}
    </style>
</head>
<body>
    <header class="header">
        <img src="max.png" alt="Max Vladymyrov">
        <div class="header-text">
            <h1>Max Vladymyrov</h1>
            <p>Senior Research Scientist, Google DeepMind</p>
            <div class="social-icons">
                <a href="https://twitter.com/mvladymyrov"><i class="fab fa-twitter"></i></a>
                <a href="https://www.linkedin.com/in/max-vladymyrov/"><i class="fab fa-linkedin"></i></a>
                <a href="https://scholar.google.com/citations?user=pQZCrqcAAAAJ&hl=en"><i class="fas fa-graduation-cap"></i></a>
            </div>
        </div>
    </header>

    <section class="about">
        <h2>About</h2>
        <p>
            At Google DeepMind, I focus on developing novel machine learning architectures to advance in-context learning and mechanistic interpretability. My goal is to create efficient, interpretable architectures that enhance AI adaptability and trustworthiness.
        </p>
        <p>
            Prior to joining DeepMind, I spent two years at Yahoo Labs. I completed my PhD at <a href="http://eecs.ucmerced.edu/">UC Merced</a> under the supervision of <a href="http://faculty.ucmerced.edu/mcarreira-perpinan/"> Miguel Á. Carreira-Perpiñán</a>, focusing on large-scale dimensionality reduction problems. I hold two master's degrees in Computer Science and International Economic Relations, and a bachelor's degree in Applied Mathematics, all from <a href="http://www.univer.kharkov.ua/en"> Kharkiv National University</a> in Ukraine.
        </p>
        <!-- <a href="#" class="cta-button">Download CV</a> -->
    </section>
	
<section class="research-statement">
    <h2>Research Statement</h2>
    <p>With over a decade of experience in machine learning research, my current work focuses on in-context learning and mechanistic interpretability. This builds upon my foundation in meta-learning, nonlinear optimization, and manifold learning. I aim to develop novel, efficient architectures that enhance AI adaptability and interpretability. My research spans from creating transformer-based solvers for large-scale linear systems to investigating the internal workings of language models. Through this work, I strive to advance AI systems that are not only more powerful and efficient, but also more transparent and aligned with human values.</p>
</section>

    <section class="publications">
		<section class="publications">
		    <h2>Publications</h2>
<div class="publication-filters">
    <!-- <div class="filter-group">
        <h4>Year</h4>
        <div class="button-row">
            <button class="filter-btn active" data-filter="all-years">All Years</button>
            <button class="filter-btn" data-filter="2024">2024</button>
            <button class="filter-btn" data-filter="2023">2023</button>
            <button class="filter-btn" data-filter="2022">2022</button>
            <button class="filter-btn" data-filter="2021">2021</button>
            <button class="filter-btn" data-filter="2019">2019</button>
            <button class="filter-btn" data-filter="2017">2017</button>
            <button class="filter-btn" data-filter="2016">2016</button>
            <button class="filter-btn" data-filter="2015">2015</button>
            <button class="filter-btn" data-filter="2014">2014</button>
            <button class="filter-btn" data-filter="2013">2013</button>
            <button class="filter-btn" data-filter="2012">2012</button>
        </div>
    </div> -->
    
    <div class="filter-group">
        <h4>Topic</h4>
        <div class="button-row">
            <button class="filter-btn active" data-filter="all-topics">All Topics</button>
            <button class="filter-btn" data-filter="tag-in-context-learning">In-context learning</button>
			<button class="filter-btn" data-filter="tag-mechanistic-interpretability">mechanistic-interpretability</button>
			<button class="filter-btn" data-filter="tag-algorithm-discovery">algorithm-discovery</button>
			<button class="filter-btn" data-filter="tag-few-shot-learning">few-shot-learning</button>
			<button class="filter-btn" data-filter="tag-hypernetworks">hypernetworks</button>
			<button class="filter-btn" data-filter="tag-nonlinear-optimization">nonlinear-optimization</button>
			<button class="filter-btn" data-filter="tag-model-adaptation">model-adaptation</button>
            <button class="filter-btn" data-filter="tag-learning-to-learn">learning-to-learn</button>
            <button class="filter-btn" data-filter="tag-meta-learning">meta-learning</button>
            <button class="filter-btn" data-filter="tag-manifold-learning">manifold-learning</button>
        </div>
    </div>
    
    <div class="filter-group">
        <h4>Venue</h4>
        <div class="button-row">
            <button class="filter-btn active" data-filter="all-venues">All Venues</button>
            <button class="filter-btn" data-filter="venue-icml">ICML</button>
            <button class="filter-btn" data-filter="venue-neurips">NeurIPS</button>
            <button class="filter-btn" data-filter="venue-cvpr">CVPR</button>
            <button class="filter-btn" data-filter="venue-iclr">ICLR</button>
            <button class="filter-btn" data-filter="venue-aistats">AISTATS</button>
            <button class="filter-btn" data-filter="venue-ecml-pkdd">ECML-PKDD</button>
            <button class="filter-btn" data-filter="venue-ijcnn">IJCNN</button>
            <button class="filter-btn" data-filter="venue-tmlr">TMLR</button>
            <button class="filter-btn" data-filter="venue-preprint">Preprint</button>
            <button class="filter-btn" data-filter="venue-thesis">Thesis</button>
        </div>
    </div>
</div>
<div class="publication-grid">
    <div class="publication-card" data-year="2024" data-tags="in-context-learning,mechanistic-interpretability,algorithm-discovery">
        <div class="venue-tag">ICML Workshop</div>
        <h3>Learning and Unlearning of Fabricated Knowledge in Language Models</h3>
        <p class="authors">Chen Sun, Nolan Miller, Andrey Zhmoginov, Max Vladymyrov, Mark Sandler</p>
        <p class="venue">Mechanistic Interpretability Workshop, ICML 2024</p>
        <div class="publication-links">
            <a href="https://openreview.net/forum?id=R5Q5lANcjY" class="pub-link"><i class="fas fa-file-pdf"></i> OpenReview</a>
        </div>
        <div class="tags">
            <span class="tag">in-context-learning</span>
            <span class="tag">mechanistic-interpretability</span>
            <span class="tag">algorithm-discovery</span>
        </div>
    </div>
<div class="publication-card" data-year="2024" data-tags="in-context-learning,mechanistic-interpretability,algorithm-discovery">
    <div class="venue-tag">ICML Workshop</div>
    <h3>Efficient Linear System Solver with Transformers</h3>
    <p class="authors">Max Vladymyrov, Johannes von Oswald, Nolan Miller, Mark Sandler</p>
    <p class="venue">AI for Math Workshop, ICML 2024</p>
    <div class="publication-links">
        <a href="https://openreview.net/forum?id=qc2adlhAWF" class="pub-link"><i class="fas fa-file-pdf"></i> OpenReview</a>
    </div>			
    <div class="tags">
        <span class="tag">in-context-learning</span>
        <span class="tag">mechanistic-interpretability</span>
        <span class="tag">algorithm-discovery</span>
    </div>
</div>

<div class="publication-card" data-year="2024" data-tags="in-context-learning,mechanistic-interpretability,algorithm-discovery">
    <div class="venue-tag">ICML Workshop</div>
    <h3>Linear Transformers Are Versatile In-Context Learners</h3> 
    <p class="authors">Max Vladymyrov, Johannes von Oswald, Mark Sandler, Rong Ge</p>
    <p class="venue">Workshop on In-Context Learning, ICML 2024</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2402.14180" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
        <a href="https://openreview.net/forum?id=MWV9zfgW9s" class="pub-link"><i class="fas fa-file-pdf"></i> OpenReview</a>
    </div>			
    <div class="tags">
        <span class="tag">in-context-learning</span>
        <span class="tag">mechanistic-interpretability</span>
        <span class="tag">algorithm-discovery</span>
    </div>
</div>

<div class="publication-card" data-year="2024" data-tags="in-context-learning,mechanistic-interpretability,algorithm-discovery">
    <div class="venue-tag">ICML Workshop</div>
    <h3>Learning Fast and Slow: Representations for In-Context Weight Modulation</h3> 
    <p class="authors">Andrey Zhmoginov, Jihwan Lee, Max Vladymyrov, Mark Sandler</p>
    <p class="venue">Workshop on In-Context Learning, ICML 2024 </p>
    <div class="publication-links">
        <a href="https://openreview.net/forum?id=XDzS9lCfQc" class="pub-link"><i class="fas fa-file-pdf"></i> OpenReview</a>
    </div>			
    <div class="tags">
        <span class="tag">in-context-learning</span>
        <span class="tag">mechanistic-interpretability</span>
        <span class="tag">algorithm-discovery</span>
    </div>
</div>

<div class="publication-card" data-year="2024" data-tags="few-shot-learning,hypernetworks">
    <div class="venue-tag">TMLR</div>
    <h3>Continual HyperTransformer: A Meta-Learner for Continual Few-Shot Learning</h3> 
    <p class="authors">Max Vladymyrov, Andrey Zhmoginov, Mark Sandler</p>
    <p class="venue">Transactions on Machine Learning Research, 2024</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2301.04584" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
        <a href="https://openreview.net/forum?id=zdtSqZnkx1" class="pub-link"><i class="fas fa-file-pdf"></i> OpenReview</a>
	<a href="papers/cht_poster.pdf" class="pub-link"><i class="fas fa-image"></i> Poster</a>
    </div>
    <div class="tags">
        <span class="tag">few-shot-learning</span>
        <span class="tag">hypernetworks</span>
    </div>
</div>

<div class="publication-card" data-year="2023" data-tags="in-context-learning,mechanistic-interpretability,algorithm-discovery">
    <div class="venue-tag">Preprint</div>
    <h3>Uncovering Mesa-Optimization Algorithms in Transformers</h3> 
    <p class="authors">Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, João Sacramento</p>
    <p class="venue">arXiv:2309.05858</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2309.05858" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
    </div>
    <div class="tags">
        <span class="tag">in-context-learning</span>
        <span class="tag">mechanistic-interpretability</span>
        <span class="tag">algorithm-discovery</span>
    </div>
</div>

<div class="publication-card" data-year="2023" data-tags="in-context-learning,mechanistic-interpretability,algorithm-discovery">
    <div class="venue-tag">ICML</div>
    <h3>Transformers Learn In-Context by Gradient Descent</h3> 
    <p class="authors">Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov</p>
    <p class="venue">International Conference on Machine Learning (ICML 2023)</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2212.07677" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
    </div>
    <div class="tags">
        <span class="tag">in-context-learning</span>
        <span class="tag">mechanistic-interpretability</span>
        <span class="tag">algorithm-discovery</span>
    </div>
</div>

<div class="publication-card" data-year="2023" data-tags="nonlinear-optimization,model-adaptation">
    <div class="venue-tag">Preprint</div>
    <h3>Training Trajectories, Mini-Batch Losses and the Curious Role of the Learning Rate</h3> 
    <p class="authors">Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, Nolan Miller</p>
    <p class="venue">arXiv preprint arXiv:2301.02312</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2301.02312" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
    </div>
    <div class="tags">
        <span class="tag">nonlinear-optimization</span>
        <span class="tag">model-adaptation</span>
    </div>
</div>

<div class="publication-card" data-year="2023" data-tags="meta-learning,learning-to-learn">
    <div class="venue-tag">CVPR</div>
    <h3>Decentralized Learning with Multi-Headed Distillation</h3> 
    <p class="authors">Andrey Zhmoginov, Mark Sandler, Nolan Miller, Gus Kristiansen, Max Vladymyrov</p>
    <p class="venue">Computer Vision and Pattern Recognition (CVPR 2023)</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2211.15774" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
    </div>
    <div class="tags">
        <span class="tag">meta-learning</span>
        <span class="tag">learning-to-learn</span>
    </div>
</div>

<div class="publication-card" data-year="2022" data-tags="few-shot-learning,hypernetworks">
    <div class="venue-tag">ICML</div>			
    <h3>HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning</h3> 
    <p class="authors">Andrey Zhmoginov, Mark Sandler, Max Vladymyrov</p>
    <p class="venue">International Conference on Machine Learning (ICML 2022)</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2201.04182" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
    </div>
    <div class="tags">
        <span class="tag">few-shot-learning</span>
        <span class="tag">hypernetworks</span>
    </div>
</div>

<div class="publication-card" data-year="2022" data-tags="nonlinear-optimization,model-adaptation">
    <div class="venue-tag">ICLR</div>
    <h3>GradMax: Growing Neural Networks Using Gradient Information</h3> 
    <p class="authors">Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Max Vladymyrov, Fabian Pedregosa</p>
    <p class="venue">International Conference on Learning Representations (ICLR 2022)</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2201.05125" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
    </div>
    <div class="tags">
        <span class="tag">nonlinear-optimization</span>
        <span class="tag">model-adaptation</span>
    </div>
</div>

<div class="publication-card" data-year="2022" data-tags="nonlinear-optimization,model-adaptation">
    <div class="venue-tag">CVPR</div>			
    <h3>Fine-Tuning Image Transformers Using Learnable Memory</h3> 
    <p class="authors">Mark Sandler, Andrey Zhmoginov, Max Vladymyrov, Andrew Jackson</p>
    <p class="venue">Computer Vision and Pattern Recognition (CVPR 2022)</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2203.15243" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
    </div>
    <div class="tags">
        <span class="tag">nonlinear-optimization</span>
        <span class="tag">model-adaptation</span>
    </div>
</div>

<div class="publication-card" data-year="2021" data-tags="meta-learning,learning-to-learn">
    <div class="venue-tag">ICML</div>			
    <h3>Meta-Learning Bidirectional Update Rules</h3> 
    <p class="authors">Mark Sandler, Max Vladymyrov, Andrey Zhmoginov, Nolan Miller, Andrew Jackson, Tom Madams, Blaise Agüera y Arcas</p>
    <p class="venue">38th International Conference on Machine Learning (ICML 2021), pp. 9288-9300</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/2104.04657" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
        <a href="papers/BLUR_poster.pdf" class="pub-link"><i class="fas fa-image"></i> Poster</a>
        <a href="papers/BLUR_ICML_presentation.pdf" class="pub-link"><i class="fas fa-file-powerpoint"></i> Slides</a>
        <a href="https://github.com/google-research/google-research/tree/master/blur" class="pub-link"><i class="fab fa-github"></i> GitHub</a>
    </div>
    <div class="tags">
        <span class="tag">meta-learning</span>
        <span class="tag">learning-to-learn</span>
    </div>
</div>

<div class="publication-card" data-year="2019" data-tags="manifold-learning">
    <div class="venue-tag">NeurIPS</div>			
    <h3>No Pressure! Addressing the Problem of Local Minima in Manifold Learning Algorithms</h3> 
    <p class="authors">Max Vladymyrov</p>
    <p class="venue">33th Annual Conference on Neural Information Processing Systems (NeurIPS 2019), pp. 678-687</p>
    <div class="publication-links">
        <a href="https://arxiv.org/abs/1906.11389" class="pub-link"><i class="fas fa-file-pdf"></i> arXiv</a>
        <a href="pp_poster.pdf" class="pub-link"><i class="fas fa-image"></i> Poster</a>
        <a href="pp_slides.pdf" class="pub-link"><i class="fas fa-file-powerpoint"></i> Slides</a>
        <a href="pp_code.zip" class="pub-link"><i class="fas fa-file-archive"></i> Matlab Code</a>
    </div>
    <div class="tags">
        <span class="tag">manifold-learning</span>
    </div>
</div>



<div class="publication-card" data-year="2017" data-tags="manifold-learning">
    <div class="venue-tag">IJCNN</div>			
    <h3>Fast, Accurate Spectral Clustering Using Locally Linear Landmarks</h3> 
    <p class="authors">Max Vladymyrov, Miguel Á. Carreira-Perpiñán</p>
    <p class="venue">30th International Joint Conference on Neural Networks (IJCNN 2017), pp. 3870-3879</p>
    <div class="publication-links">
        <a href="http://www.ijcnn.org/" class="pub-link"><i class="fas fa-external-link-alt"></i> Conference</a>
        <a href="papers/ijcnn17.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Paper</a>
        <a href="papers/ijcnn17-slides.pdf" class="pub-link"><i class="fas fa-file-powerpoint"></i> Slides</a>
    </div>
    <div class="tags">
        <span class="tag">manifold-learning</span>
    </div>
</div>

<div class="publication-card" data-year="2016" data-tags="manifold-learning">
    <div class="venue-tag">ICML</div>			
    <h3>The Variational Nyström Method for Large-Scale Spectral Problems</h3> 
    <p class="authors">Max Vladymyrov, Miguel Á. Carreira-Perpiñán</p>
    <p class="venue">33th International Conference on Machine Learning (ICML 2016)</p>
    <div class="publication-links">
        <a href="http://jmlr.org/proceedings/papers/v48/vladymyrov16.html" class="pub-link"><i class="fas fa-external-link-alt"></i> Paper</a>
        <a href="papers/icml16.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Preprint</a>
        <a href="papers/icml16-supp.pdf" class="pub-link"><i class="fas fa-file-alt"></i> Supplementary</a>
        <a href="http://icml.cc/2016/reviews/128.txt" class="pub-link"><i class="fas fa-comments"></i> Reviews</a>
        <a href="papers/icml16-poster.pdf" class="pub-link"><i class="fas fa-image"></i> Poster</a>
        <a href="papers/icml16-slides.pdf" class="pub-link"><i class="fas fa-file-powerpoint"></i> Slides</a>
        <a href="vn.tar.gz" class="pub-link"><i class="fas fa-file-archive"></i> Matlab Code</a>
    </div>
    <div class="tags">
        <span class="tag">manifold-learning</span>
    </div>
</div>

<div class="publication-card" data-year="2015" data-tags="manifold-learning">
    <div class="venue-tag">NeurIPS</div>			
    <h3>A Fast, Universal Algorithm to Learn Parametric Nonlinear Embeddings</h3> 
    <p class="authors">Miguel Á. Carreira-Perpiñán, Max Vladymyrov</p>
    <p class="venue">29th Annual Conference on Neural Information Processing Systems (NIPS 2015), pp. 253-261</p>
    <div class="publication-links">
        <a href="https://nips.cc/Conferences/2015/AcceptedPapers" class="pub-link"><i class="fas fa-external-link-alt"></i> Conference</a>
        <a href="papers/nips15.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Preprint</a>
        <a href="papers/nips15-supp.zip" class="pub-link"><i class="fas fa-file-archive"></i> Supplementary</a>
        <a href="https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips28/reviews/134.html" class="pub-link"><i class="fas fa-comments"></i> Reviews</a>
        <a href="papers/nips15-poster.pdf" class="pub-link"><i class="fas fa-image"></i> Poster</a>
        <a href="pe_mac.zip" class="pub-link"><i class="fas fa-file-archive"></i> Matlab Code</a>
    </div>
    <div class="tags">
        <span class="tag">manifold-learning</span>
    </div>
</div>

<div class="publication-card" data-year="2014" data-tags="manifold-learning">
    <div class="venue-tag">AISTATS</div>			
    <h3>Linear-Time Training of Nonlinear Low-Dimensional Embeddings</h3> 
    <p class="authors">Max Vladymyrov, Miguel Á. Carreira-Perpiñán</p>
    <p class="venue">17th International Conference on Artificial Intelligence and Statistics (AISTATS 2014), pp. 968-977</p>
    <div class="publication-links">
        <a href="http://jmlr.org/proceedings/papers/v33/vladymyrov14.html" class="pub-link"><i class="fas fa-external-link-alt"></i> Paper</a>
        <a href="papers/aistats14b.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Preprint</a>
        <a href="papers/aistats14-supp/index.html" class="pub-link"><i class="fas fa-file-alt"></i> Supplementary</a>
        <a href="papers/aistats14-poster.pdf" class="pub-link"><i class="fas fa-image"></i> Poster</a>
        <a href="NBody_NLE.tar.gz" class="pub-link"><i class="fas fa-file-archive"></i> Matlab Code</a>
    </div>
    <div class="tags">
        <span class="tag">manifold-learning</span>
    </div>
</div>

<div class="publication-card" data-year="2013" data-tags="manifold-learning">
    <div class="venue-tag">ECML-PKDD</div>			
    <h3>Locally Linear Landmarks for Large-Scale Manifold Learning</h3> 
    <p class="authors">Max Vladymyrov, Miguel Á. Carreira-Perpiñán</p>
    <p class="venue">24th European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2013), pp. 256-271</p>
    <div class="publication-links">
        <a href="http://dx.doi.org/10.1007/978-3-642-40994-3_17" class="pub-link"><i class="fas fa-external-link-alt"></i> Paper</a>
        <a href="papers/ecml13.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Preprint</a>
        <a href="papers/ecml13-poster.pdf" class="pub-link"><i class="fas fa-image"></i> Poster</a>
        <a href="papers/ecml13-slides.pdf" class="pub-link"><i class="fas fas fa-file-powerpoint"></i> Slides</a>
        <a href="LLL.tar.gz" class="pub-link"><i class="fas fa-file-archive"></i> Matlab Code</a>
    </div>
    <div class="tags">
        <span class="tag">manifold-learning</span>
    </div>
</div>

<div class="publication-card" data-year="2013" data-tags="manifold-learning">
    <div class="venue-tag">ICML</div>
    <h3>Entropic Affinities: Properties and Efficient Numerical Computation</h3> 
    <p class="authors">Max Vladymyrov, Miguel Á. Carreira-Perpiñán</p>
    <p class="venue">30th International Conference on Machine Learning (ICML 2013), pp. 477-485</p>
    <div class="publication-links">
        <a href="http://jmlr.org/proceedings/papers/v28/vladymyrov13.html" class="pub-link"><i class="fas fa-external-link-alt"></i> Paper</a>
        <a href="papers/icml13.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Preprint</a>
        <a href="papers/icml13-supp/index.html" class="pub-link"><i class="fas fa-file-alt"></i> Supplementary</a>
        <a href="papers/icml13-slides.pdf" class="pub-link"><i class="fas fa-file-powerpoint"></i> Slides</a>
        <a href="papers/icml13-poster.pdf" class="pub-link"><i class="fas fa-image"></i> Poster</a>
        <a href="ea.tar.gz" class="pub-link"><i class="fas fa-file-archive"></i> Matlab Code</a>
    </div>
    <div class="tags">
        <span class="tag">manifold-learning</span>
    </div>
</div>

<div class="publication-card" data-year="2012" data-tags="manifold-learning">
    <div class="venue-tag">ICML</div>
    <h3>Partial-Hessian Strategies for Fast Learning of Nonlinear Embeddings</h3> 
    <p class="authors">Max Vladymyrov, Miguel Á. Carreira-Perpiñán</p>
    <p class="venue">29th International Conference on Machine Learning (ICML 2012), pp. 345-352</p>
    <div class="publication-links">
        <a href="http://icml.cc/2012/papers/199.pdf" class="pub-link"><i class="fas fa-external-link-alt"></i> Paper</a>
        <a href="papers/icml12.pdf" class="pub-link"><i class="fas fa-file-pdf"></i> Preprint</a>
        <a href="papers/icml12-supp/index.html" class="pub-link"><i class="fas fa-file-alt"></i> Supplementary</a>
        <a href="papers/icml12-slides.pdf" class="pub-link"><i class="fas fa-file-powerpoint"></i> Slides</a>
        <a href="papers/icml12-poster.pdf" class="pub-link"><i class="fas fa-image"></i> Poster</a>
        <a href="code-EE_SNE_tSNE.tar.gz" class="pub-link"><i class="fas fa-file-archive"></i> Matlab Code</a>
    </div>
    <div class="tags">
        <span class="tag">manifold-learning</span>
    </div>
</div>

</section>

    <footer>
        <div class="footer-content">
            <p>&copy; 2024 Max Vladymyrov.</p>
        </div>
        <button id="back-to-top" title="Back to Top"><i class="fas fa-arrow-up"></i></button>
    </footer>

    <script>
        // JavaScript for interactive elements
		document.addEventListener('DOMContentLoaded', function() {
		    const filterBtns = document.querySelectorAll('.filter-btn');
		    const publicationCards = document.querySelectorAll('.publication-card');

		    filterBtns.forEach(btn => {
		        btn.addEventListener('click', () => {
		            const filter = btn.getAttribute('data-filter');
		            const filterGroup = btn.closest('.filter-group');
            
		            // Remove active class from all buttons in this group
		            filterGroup.querySelectorAll('.filter-btn').forEach(b => b.classList.remove('active'));
		            // Add active class to clicked button
		            btn.classList.add('active');

		            if (filter === 'all-years' || filter === 'all-topics' || filter === 'all-venues') {
		                // If "All" is selected, show all cards
		                publicationCards.forEach(card => card.style.display = 'block');
		            } else if (filter.startsWith('tag-')) {
		                const tag = filter.replace('tag-', '');
		                publicationCards.forEach(card => {
		                    const cardTags = card.getAttribute('data-tags').split(',');
		                    card.style.display = cardTags.includes(tag) ? 'block' : 'none';
		                });
		            } else if (filter.startsWith('venue-')) {
		                const venue = filter.replace('venue-', '').toUpperCase();
		                publicationCards.forEach(card => {
		                    const cardVenue = card.querySelector('.venue-tag').textContent.toUpperCase();
		                    card.style.display = cardVenue.includes(venue) ? 'block' : 'none';
		                });
		            } else {
		                // Year filter
		                publicationCards.forEach(card => {
		                    card.style.display = card.getAttribute('data-year') === filter ? 'block' : 'none';
		                });
		            }
		        });
		    });
            // Back to top button
            const backToTopButton = document.getElementById('back-to-top');
            window.addEventListener('scroll', () => {
                if (window.pageYOffset > 300) {
                    backToTopButton.style.display = 'block';
                } else {
                    backToTopButton.style.display = 'none';
                }
            });

            backToTopButton.addEventListener('click', () => {
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        });
    </script>
</body>
</html>
